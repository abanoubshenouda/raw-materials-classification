# -*- coding: utf-8 -*-
"""Preprocessing(TensorFlow).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X_Efjlw1y4i39v6dB1Qfs6gCWVjCGn3m

**Upload Dataset**
"""

!pip install -q kaggle

!apt-get -qq install -y unzip



!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json



!kaggle datasets download -d haaroonafroz/material-dataset-new

!unzip -q material-dataset-new.zip -d dataset_raw

"""**Import Libraries**"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from pathlib import Path
import cv2
from sklearn.model_selection import train_test_split
import random

print("TensorFlow version:", tf.__version__)
print("GPU Available:", tf.config.list_physical_devices('GPU'))

"""**Explore Dataset Structure**"""

# Set dataset path
dataset_path = 'dataset_raw'

# Explore directory structure
for root, dirs, files in os.walk(dataset_path):
    level = root.replace(dataset_path, '').count(os.sep)
    indent = ' ' * 2 * level
    print(f'{indent}{os.path.basename(root)}/')

    # Only show first 3 files per directory
    subindent = ' ' * 2 * (level + 1)
    for file in files[:3]:
        print(f'{subindent}{file}')
    if len(files) > 3:
        print(f'{subindent}... and {len(files) - 3} more files')

"""**Count Images Per Class**"""

# Get all class directories and count images
class_counts = {}
class_dirs = []

for item in os.listdir("/content/dataset_raw/Material_dataset"):
    item_path = os.path.join("/content/dataset_raw/Material_dataset", item)
    if os.path.isdir(item_path):
        # Count image files
        image_files = [f for f in os.listdir(item_path)
                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        class_counts[item] = len(image_files)
        class_dirs.append(item)

# Display class distribution
print("Class Distribution:")
print("-" * 40)
for class_name, count in sorted(class_counts.items()):
    print(f"{class_name:20s}: {count:4d} images")
print("-" * 40)
print(f"Total Classes: {len(class_counts)}")
print(f"Total Images: {sum(class_counts.values())}")

# Visualize distribution
plt.figure(figsize=(12, 6))
plt.bar(class_counts.keys(), class_counts.values(), color='steelblue')
plt.xlabel('Material Class', fontsize=12)
plt.ylabel('Number of Images', fontsize=12)
plt.title('Dataset Class Distribution', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**Understand Dataset Structure & Load Annotations**"""

import json

# Load annotations to understand the dataset better
with open('dataset_raw/Material_dataset/annotations.json', 'r') as f:
    annotations = json.load(f)

print("Annotation keys:", annotations.keys() if isinstance(annotations, dict) else "List format")
print("\nSample annotation:")
if isinstance(annotations, dict):
    sample_key = list(annotations.keys())[0]
    print(f"{sample_key}: {annotations[sample_key]}")
elif isinstance(annotations, list):
    print(annotations[0])

"""**Extract Class Labels from Filenames**"""

# Define material classes based on filename suffixes
material_mapping = {
    '_m.': 'metal',
    '_w.': 'wood',
    '_g.': 'glass',
    '_p.': 'plastic',
    '_mx.': 'mixed'
}

# Get all image paths and extract labels
image_dir = 'dataset_raw/Material_dataset/JPEGImages'
image_files = os.listdir(image_dir)

# Create dataset dictionary
dataset_info = []

for img_file in image_files:
    # Extract class from filename
    label = None
    for suffix, class_name in material_mapping.items():
        if suffix in img_file:
            label = class_name
            break

    if label:
        dataset_info.append({
            'image_path': os.path.join(image_dir, img_file),
            'filename': img_file,
            'label': label
        })

print(f"Total valid images: {len(dataset_info)}")

# Count samples per class
from collections import Counter
label_counts = Counter([item['label'] for item in dataset_info])

print("\nClass Distribution:")
print("-" * 40)
for label, count in sorted(label_counts.items()):
    print(f"{label:15s}: {count:5d} images ({count/len(dataset_info)*100:.1f}%)")
print("-" * 40)

# Visualize
plt.figure(figsize=(10, 6))
labels, counts = zip(*sorted(label_counts.items()))
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
bars = plt.bar(labels, counts, color=colors[:len(labels)], edgecolor='black', linewidth=1.5)
plt.xlabel('Material Class', fontsize=12, fontweight='bold')
plt.ylabel('Number of Images', fontsize=12, fontweight='bold')
plt.title('Raw Materials Dataset - Class Distribution', fontsize=14, fontweight='bold')
plt.grid(axis='y', alpha=0.3, linestyle='--')

# Add count labels on bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{int(height)}',
             ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

"""**Visualize Sample Images from Each Class**"""

# Select random samples from each class
fig, axes = plt.subplots(len(label_counts), 5, figsize=(15, 3*len(label_counts)))
fig.suptitle('Sample Images from Each Material Class', fontsize=16, fontweight='bold')

for idx, (label, count) in enumerate(sorted(label_counts.items())):
    # Get images for this class
    class_images = [item for item in dataset_info if item['label'] == label]
    samples = random.sample(class_images, min(5, len(class_images)))

    for col, sample in enumerate(samples):
        img = plt.imread(sample['image_path'])

        if len(label_counts) > 1:
            ax = axes[idx, col]
        else:
            ax = axes[col]

        ax.imshow(img)
        ax.axis('off')
        if col == 0:
            ax.set_title(f"{label.upper()}", fontsize=12, fontweight='bold', loc='left')

plt.tight_layout()
plt.show()

"""**Analyze Image Properties**"""

# Analyze image dimensions and properties
print("Analyzing image properties (sampling 100 images)...")

sample_images = random.sample(dataset_info, min(100, len(dataset_info)))
dimensions = []
aspect_ratios = []

for item in sample_images:
    img = plt.imread(item['image_path'])
    h, w = img.shape[:2]
    dimensions.append((h, w))
    aspect_ratios.append(w/h)

# Statistics
heights, widths = zip(*dimensions)
print("\nImage Dimensions:")
print(f"  Height - Min: {min(heights)}, Max: {max(heights)}, Avg: {np.mean(heights):.0f}")
print(f"  Width  - Min: {min(widths)}, Max: {max(widths)}, Avg: {np.mean(widths):.0f}")
print(f"  Aspect Ratio - Min: {min(aspect_ratios):.2f}, Max: {max(aspect_ratios):.2f}, Avg: {np.mean(aspect_ratios):.2f}")

# Visualize
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Dimension scatter
axes[0].scatter(widths, heights, alpha=0.5, c='steelblue', edgecolors='black')
axes[0].set_xlabel('Width (pixels)', fontsize=11, fontweight='bold')
axes[0].set_ylabel('Height (pixels)', fontsize=11, fontweight='bold')
axes[0].set_title('Image Dimensions Distribution', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Aspect ratio histogram
axes[1].hist(aspect_ratios, bins=20, color='coral', edgecolor='black', alpha=0.7)
axes[1].set_xlabel('Aspect Ratio (W/H)', fontsize=11, fontweight='bold')
axes[1].set_ylabel('Frequency', fontsize=11, fontweight='bold')
axes[1].set_title('Aspect Ratio Distribution', fontsize=12, fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# Recommended image size
recommended_size = (224, 224)  # Standard for transfer learning
print(f"\nâœ“ Recommended preprocessing size: {recommended_size}")

"""**Split Dataset (Train/Val/Test)**"""

from sklearn.model_selection import train_test_split

# Convert to arrays for splitting
image_paths = [item['image_path'] for item in dataset_info]
labels = [item['label'] for item in dataset_info]

# Create label to index mapping
unique_labels = sorted(list(set(labels)))
label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
idx_to_label = {idx: label for label, idx in label_to_idx.items()}

print("Label Mapping:")
for label, idx in label_to_idx.items():
    print(f"  {idx}: {label}")

# Convert labels to indices
label_indices = [label_to_idx[label] for label in labels]

# Split: 70% train, 15% validation, 15% test
X_train, X_temp, y_train, y_temp = train_test_split(
    image_paths, label_indices,
    test_size=0.30,
    random_state=42,
    stratify=label_indices
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,
    random_state=42,
    stratify=y_temp
)

print(f"\nDataset Split:")
print(f"  Training:   {len(X_train):5d} images ({len(X_train)/len(image_paths)*100:.1f}%)")
print(f"  Validation: {len(X_val):5d} images ({len(X_val)/len(image_paths)*100:.1f}%)")
print(f"  Test:       {len(X_test):5d} images ({len(X_test)/len(image_paths)*100:.1f}%)")

# Verify stratification
print("\nClass distribution in splits:")
for split_name, split_labels in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
    counts = Counter(split_labels)
    print(f"\n{split_name}:")
    for idx in sorted(counts.keys()):
        print(f"  {idx_to_label[idx]:10s}: {counts[idx]:4d} ({counts[idx]/len(split_labels)*100:.1f}%)")

"""**Basic Preprocessing Functions**"""

# Image size for models (standard for transfer learning)
IMG_HEIGHT = 224
IMG_WIDTH = 224
NUM_CLASSES = len(unique_labels)

def load_and_preprocess_image(image_path, label):
    """Load image and apply basic preprocessing"""
    # Read image
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)

    # Resize
    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])

    # Normalize to [0, 1]
    img = img / 255.0

    return img, label

# Test the function
test_img, test_label = load_and_preprocess_image(X_train[0], y_train[0])
print(f"Preprocessed image shape: {test_img.shape}")
print(f"Pixel value range: [{tf.reduce_min(test_img):.3f}, {tf.reduce_max(test_img):.3f}]")
print(f"Label: {test_label} ({idx_to_label[test_label]})")

# Visualize
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

original = plt.imread(X_train[0])
axes[0].imshow(original)
axes[0].set_title(f'Original Image\nShape: {original.shape}', fontweight='bold')
axes[0].axis('off')

axes[1].imshow(test_img)
axes[1].set_title(f'Preprocessed Image\nShape: {test_img.shape}, Normalized', fontweight='bold')
axes[1].axis('off')

plt.tight_layout()
plt.show()

"""**ðŸŽ¯ BONUS - Advanced Lighting Augmentation Functions**"""

def random_brightness_adjustment(image, max_delta=0.2):
    """Simulate varying illumination conditions - FIXED"""
    image = tf.image.random_brightness(image, max_delta=max_delta)
    return tf.clip_by_value(image, 0.0, 1.0)

def random_contrast_adjustment(image, lower=0.7, upper=1.3):
    """Simulate different lighting contrasts - FIXED"""
    image = tf.image.random_contrast(image, lower=lower, upper=upper)
    return tf.clip_by_value(image, 0.0, 1.0)

def simulate_shadow(image, shadow_intensity=0.3):
    """
    Add realistic shadow overlay to simulate factory lighting conditions
    FIXED: Reduced intensity and added proper clipping
    """
    # Get image shape
    shape = tf.shape(image)
    height = shape[0]
    width = shape[1]

    # Random shadow parameters - REDUCED intensity
    shadow_strength = tf.random.uniform([], 0.15, shadow_intensity)

    # Create gradient shadow mask
    x = tf.cast(tf.range(width), tf.float32) / tf.cast(width, tf.float32)
    y = tf.cast(tf.range(height), tf.float32) / tf.cast(height, tf.float32)

    # Random shadow direction
    direction = tf.random.uniform([], 0, 4, dtype=tf.int32)

    if direction == 0:  # Top-down
        shadow_mask = tf.expand_dims(y, 1)
    elif direction == 1:  # Left-right
        shadow_mask = tf.expand_dims(x, 0)
    elif direction == 2:  # Diagonal
        xx, yy = tf.meshgrid(x, y)
        shadow_mask = (xx + yy) / 2.0
    else:  # Radial
        center_x = tf.random.uniform([], 0.3, 0.7)
        center_y = tf.random.uniform([], 0.3, 0.7)
        xx, yy = tf.meshgrid(x, y)
        shadow_mask = tf.sqrt((xx - center_x)**2 + (yy - center_y)**2)

    # Normalize and apply
    shadow_mask = tf.expand_dims(shadow_mask, -1)
    shadow_mask = shadow_mask / tf.reduce_max(shadow_mask)
    shadow_mask = 1.0 - (shadow_mask * shadow_strength)

    result = image * shadow_mask
    return tf.clip_by_value(result, 0.0, 1.0)

def simulate_glare(image, num_spots=2, intensity=0.4):
    """
    Add bright spots to simulate reflections/glare from factory lights
    FIXED: Reduced intensity and spots
    """
    shape = tf.shape(image)
    height = tf.cast(shape[0], tf.float32)
    width = tf.cast(shape[1], tf.float32)

    # Create glare mask
    glare_mask = tf.zeros_like(image)

    for _ in range(num_spots):
        # Random glare spot position
        center_x = tf.random.uniform([], 0, 1)
        center_y = tf.random.uniform([], 0, 1)
        radius = tf.random.uniform([], 0.04, 0.12)

        # Create coordinate grid
        x = tf.linspace(0.0, 1.0, shape[1])
        y = tf.linspace(0.0, 1.0, shape[0])
        xx, yy = tf.meshgrid(x, y)

        # Gaussian spot
        dist = tf.sqrt((xx - center_x)**2 + (yy - center_y)**2)
        spot = tf.exp(-(dist**2) / (2 * radius**2))
        spot = tf.expand_dims(spot, -1)

        glare_mask = glare_mask + spot

    # Normalize and apply - FIXED intensity
    glare_mask = glare_mask / (tf.reduce_max(glare_mask) + 1e-7)
    glare_strength = tf.random.uniform([], 0.15, intensity)

    return tf.clip_by_value(image + glare_mask * glare_strength, 0.0, 1.0)

def random_lighting_augmentation(image, probability=0.7):
    """
    Apply random combination of lighting effects
    This is the KEY function for the BONUS challenge - FIXED
    """
    # Randomly decide which augmentations to apply
    apply_brightness = tf.random.uniform([]) < probability
    apply_contrast = tf.random.uniform([]) < probability
    apply_shadow = tf.random.uniform([]) < 0.4
    apply_glare = tf.random.uniform([]) < 0.25

    # Apply brightness
    if apply_brightness:
        image = random_brightness_adjustment(image)

    # Apply contrast
    if apply_contrast:
        image = random_contrast_adjustment(image)

    # Apply shadow
    if apply_shadow:
        image = simulate_shadow(image)

    # Apply glare
    if apply_glare:
        image = simulate_glare(image)

    # CRITICAL: Final clipping to ensure valid range
    image = tf.clip_by_value(image, 0.0, 1.0)

    return image

print("âœ“ Advanced lighting augmentation functions created (FIXED VERSION)!")
print("  - Random brightness adjustment (with clipping)")
print("  - Random contrast adjustment (with clipping)")
print("  - Shadow simulation (reduced intensity)")
print("  - Glare/reflection simulation (reduced intensity)")
print("  - Combined random lighting augmentation (safe ranges)")
print("\nâš ï¸  Key improvements:")
print("  â€¢ Reduced augmentation intensities for material texture preservation")
print("  â€¢ Added proper clipping at every step")
print("  â€¢ Balanced realism vs. texture visibility")

"""**ðŸŽ¯ Visualize BONUS Augmentations**"""

# Load a sample image
sample_img, _ = load_and_preprocess_image(X_train[0], y_train[0])

# Create figure showing different augmentations
fig, axes = plt.subplots(3, 4, figsize=(16, 12))
fig.suptitle('BONUS: Lighting Robustness Augmentations - Simulating Factory Conditions',
             fontsize=16, fontweight='bold')

augmentations = [
    ("Original", lambda x: x),
    ("Brightness Low", lambda x: tf.clip_by_value(tf.image.adjust_brightness(x, -0.15), 0.0, 1.0)),
    ("Brightness High", lambda x: tf.clip_by_value(tf.image.adjust_brightness(x, 0.15), 0.0, 1.0)),
    ("Low Contrast", lambda x: tf.clip_by_value(tf.image.adjust_contrast(x, 0.7), 0.0, 1.0)),
    ("High Contrast", lambda x: tf.clip_by_value(tf.image.adjust_contrast(x, 1.3), 0.0, 1.0)),
    ("Shadow (Top)", lambda x: simulate_shadow(x, 0.3)),
    ("Shadow (Radial)", lambda x: simulate_shadow(x, 0.3)),
    ("Glare (Light)", lambda x: simulate_glare(x, num_spots=2, intensity=0.3)),
    ("Glare (Strong)", lambda x: simulate_glare(x, num_spots=2, intensity=0.4)),
    ("Combined 1", lambda x: random_lighting_augmentation(x)),
    ("Combined 2", lambda x: random_lighting_augmentation(x)),
    ("Combined 3", lambda x: random_lighting_augmentation(x)),
]

for idx, (name, aug_func) in enumerate(augmentations):
    row = idx // 4
    col = idx % 4

    augmented = aug_func(sample_img)
    axes[row, col].imshow(augmented)
    axes[row, col].set_title(name, fontsize=11, fontweight='bold')
    axes[row, col].axis('off')

plt.tight_layout()
plt.show()

print("âœ“ These augmentations help the model become robust to:")
print("  â€¢ Varying illumination in factory settings")
print("  â€¢ Shadows from overhead lighting or objects")
print("  â€¢ Glare and reflections from shiny materials")
print("  â€¢ Different contrast conditions")
print("\nâœ“ All pixel values properly clipped to [0.0, 1.0] range")

"""**Additional Geometric Augmentations**"""

def geometric_augmentation(image):
    """Apply geometric transformations while preserving texture"""

    # Random flip
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)

    # Random rotation (small angles to preserve texture orientation)
    # Only 90-degree rotations for materials to preserve texture
    k = tf.random.uniform([], 0, 4, dtype=tf.int32)
    image = tf.image.rot90(image, k=k)

    # Random crop and resize (simulates different distances/scales)
    crop_size = tf.random.uniform([], 0.8, 1.0)
    crop_h = tf.cast(tf.cast(IMG_HEIGHT, tf.float32) * crop_size, tf.int32)
    crop_w = tf.cast(tf.cast(IMG_WIDTH, tf.float32) * crop_size, tf.int32)
    image = tf.image.random_crop(image, [crop_h, crop_w, 3])
    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])

    return image

def color_augmentation(image):
    """Subtle color adjustments (preserve material appearance)"""
    # Small hue adjustment
    image = tf.image.random_hue(image, max_delta=0.05)

    # Saturation adjustment
    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)

    return tf.clip_by_value(image, 0.0, 1.0)

print("âœ“ Geometric and color augmentation functions created!")

"""**Complete Training Augmentation Pipeline**"""

def augment_training_image(image, label):
    """
    Complete augmentation pipeline for TRAINING data
    Combines all augmentations with the BONUS lighting effects
    """
    # Apply geometric augmentations
    image = geometric_augmentation(image)

    # Apply color augmentations
    image = color_augmentation(image)

    # ðŸŽ¯ BONUS: Apply lighting robustness augmentations
    image = random_lighting_augmentation(image, probability=0.8)

    return image, label

def preprocess_validation_image(image, label):
    """
    Simple preprocessing for VALIDATION/TEST data (NO augmentation)
    """
    return image, label

print("âœ“ Complete augmentation pipeline created!")
print("\nTraining pipeline includes:")
print("  1. Geometric augmentation (flip, rotation, crop)")
print("  2. Color augmentation (hue, saturation)")
print("  3. ðŸŽ¯ BONUS: Lighting robustness (brightness, contrast, shadow, glare)")
print("\nValidation/Test pipeline:")
print("  - Only basic preprocessing (resize + normalize)")

"""**Create TensorFlow Datasets**"""

BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE

# Create training dataset
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = (train_dataset
    .shuffle(buffer_size=1000, seed=42)
    .map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)
    .map(augment_training_image, num_parallel_calls=AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

# Create validation dataset
val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))
val_dataset = (val_dataset
    .map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)
    .map(preprocess_validation_image, num_parallel_calls=AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

# Create test dataset
test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))
test_dataset = (test_dataset
    .map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)
    .map(preprocess_validation_image, num_parallel_calls=AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

print("âœ“ TensorFlow datasets created successfully!")
print(f"\nDataset specifications:")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Image size: {IMG_HEIGHT}x{IMG_WIDTH}")
print(f"  Number of classes: {NUM_CLASSES}")
print(f"\nDataset sizes:")
print(f"  Training batches:   {len(list(train_dataset))}")
print(f"  Validation batches: {len(list(val_dataset))}")
print(f"  Test batches:       {len(list(test_dataset))}")

"""**Visualize Augmented Training Batches**"""

# Get one batch of augmented training data
sample_batch = next(iter(train_dataset))
images, labels = sample_batch

# Display grid of augmented images
fig, axes = plt.subplots(4, 8, figsize=(20, 10))
fig.suptitle('Augmented Training Batch (With BONUS Lighting Effects)',
             fontsize=16, fontweight='bold')

for i in range(32):
    if i >= len(images):
        break
    row = i // 8
    col = i % 8

    axes[row, col].imshow(images[i])
    label_name = idx_to_label[labels[i].numpy()]
    axes[row, col].set_title(f'{label_name}', fontsize=9, fontweight='bold')
    axes[row, col].axis('off')

plt.tight_layout()
plt.show()

print("âœ“ Notice the variety in lighting, shadows, and glare!")
print("  This prepares the model for real factory conditions.")

"""**Save Preprocessing Configuration**"""

# Save important configuration
preprocessing_config = {
    'img_height': IMG_HEIGHT,
    'img_width': IMG_WIDTH,
    'num_classes': NUM_CLASSES,
    'batch_size': BATCH_SIZE,
    'label_mapping': label_to_idx,
    'idx_to_label': idx_to_label,
    'train_size': len(X_train),
    'val_size': len(X_val),
    'test_size': len(X_test),
    'augmentation_enabled': True,
    'bonus_lighting_augmentation': True
}

# Save to JSON
import json
with open('preprocessing_config.json', 'w') as f:
    json.dump(preprocessing_config, f, indent=4)

print("âœ“ Preprocessing configuration saved to 'preprocessing_config.json'")
print("\nConfiguration summary:")
for key, value in preprocessing_config.items():
    if not isinstance(value, dict):
        print(f"  {key}: {value}")

"""**Dataset Statistics & Validation**"""

# Validate dataset
print("Validating datasets...")

def check_dataset_statistics(dataset, name):
    """Check basic statistics of a dataset"""
    pixel_means = []
    pixel_stds = []

    for images, labels in dataset.take(10):
        pixel_means.append(tf.reduce_mean(images).numpy())
        pixel_stds.append(tf.math.reduce_std(images).numpy())

    print(f"\n{name} Dataset:")
    print(f"  Mean pixel value: {np.mean(pixel_means):.3f} Â± {np.std(pixel_means):.3f}")
    print(f"  Std pixel value:  {np.mean(pixel_stds):.3f} Â± {np.std(pixel_stds):.3f}")

check_dataset_statistics(train_dataset, "Training")
check_dataset_statistics(val_dataset, "Validation")
check_dataset_statistics(test_dataset, "Test")

print("\nâœ“ Dataset validation complete!")

"""**Compare Original vs Augmented**"""

# Side-by-side comparison
fig, axes = plt.subplots(2, 5, figsize=(18, 8))
fig.suptitle('Original vs Augmented Images (Lighting Robustness)',
             fontsize=14, fontweight='bold')

# Get original images
original_dataset = tf.data.Dataset.from_tensor_slices((X_train[:5], y_train[:5]))
original_dataset = original_dataset.map(load_and_preprocess_image)

# Get augmented images
augmented_dataset = tf.data.Dataset.from_tensor_slices((X_train[:5], y_train[:5]))
augmented_dataset = (augmented_dataset
    .map(load_and_preprocess_image)
    .map(augment_training_image))

# Display
for idx, ((orig_img, orig_label), (aug_img, aug_label)) in enumerate(zip(original_dataset, augmented_dataset)):
    # Original
    axes[0, idx].imshow(orig_img)
    axes[0, idx].set_title(f'Original\n{idx_to_label[orig_label.numpy()]}', fontsize=10)
    axes[0, idx].axis('off')

    # Augmented
    axes[1, idx].imshow(aug_img)
    axes[1, idx].set_title(f'Augmented\n{idx_to_label[aug_label.numpy()]}', fontsize=10)
    axes[1, idx].axis('off')

axes[0, 0].text(-0.1, 0.5, 'Original', rotation=90,
                transform=axes[0, 0].transAxes,
                fontsize=12, fontweight='bold', va='center')
axes[1, 0].text(-0.1, 0.5, 'Augmented\n(with lighting)', rotation=90,
                transform=axes[1, 0].transAxes,
                fontsize=12, fontweight='bold', va='center')

plt.tight_layout()
plt.show()